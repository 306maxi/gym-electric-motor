{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Tensorforce to train a reinforcement-learning agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as an educational introduction to the usage of Tensorforce using a gym-electric-motor (GEM) environment. The goal of this notebook is to give an understanding of what tensorforce is and how to use it to train and evaluate a reinforcement learning agent that can solve a current control problem of the GEM toolbox.\n",
    "\n",
    "The following code snippets are only needed if you are executing this file directly from a cloned GitHub repository where you don't have GEM installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path().resolve().parent.parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can start you need to make sure that you have both gym-electric-motor and tensorforce installed. You can install both easily using pip:\n",
    "\n",
    "  - pip install gym-electric-motor\n",
    "  - pip install tensorforce\n",
    "\n",
    "Alternatively, you can install their latest developer version directly from GitHub:\n",
    "\n",
    "  - [GitHub Gym-Electric-Motor](https://github.com/upb-lea/gym-electric-motor)\n",
    "  - [GitHub Tensorforce](https://github.com/tensorforce/tensorforce)\n",
    "\n",
    "You also need to make sure that numpy and gym are installed. You can install both using pip, too. After you have done that you should be able to execute the following cells without any problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Setting up the GEM-Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind reinforcement learning is to create a so-called agent, that should learn by itself to solve a specified task in a given environment. This environment gives the agent feedback on its actions an reinforces the targeted behavior.\n",
    "In this notebook, we focus on the task to train a controller for the current control of a *Permanent Magnet Synchronous Motor* (*PMSM*).\n",
    " \n",
    "In the following, we briefly present the used GEM-envrionment, but this notebook does not focus directly on the detailed usage of GEM. If you are new to the used environment and interested in finding out what it does and how to use it, we recommend taking a look at the educational notebook which is dealing with GEM.\n",
    "\n",
    "To save some space in this notebook, we will use a function defined in an external python file called **setting_environment.py**. If you want to know how we defined our environment's parameters you can take a look at that file. By simply calling the **set_env()** function from the external file, you can set up our used environment for a *PMSM* with discrete inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of the control-setup from the GEM-environment is displayed in the following figure. \n",
    "\n",
    "![Motor Setup](img/SCML_Setting.svg)\n",
    "\n",
    "The agent controls the converter who converts the supply currents to the currents flowing into the motor - for the *PMSM*: $i_{sq}$ and $i_{sd}$\n",
    "\n",
    "In the continuous case, the agent's action is controlling the converter's output voltage directly through means of duty cycling. In the discrete case, the agent's action decides which converter switches are open and which are closed. Therefore, only a discrete amount of options are available. For this environment, we are using the *Discrete B6 Bridge Converter* which has three switches which leads to a total of eight possible actions the agent can take.\n",
    "\n",
    "![Motor Setup](img/B6.svg)\n",
    "\n",
    "The motor is described through the schematic:\n",
    "\n",
    "![Motor Setup](img/ESBdq1.svg)\n",
    "\n",
    "Which leads to the electrical ODEs:\n",
    "\n",
    "<h3 align=\"center\">\n",
    "\n",
    "$\\frac{\\mathrm{d}i_{sq}}{\\mathrm{d}t} = \\frac{u_{sq}-pL_d\\omega_{me}i_{sd}-R_si_{sq}}{L_q}$\n",
    "\n",
    "$\\frac{\\mathrm{d}i_{sd}}{\\mathrm{d}t} = \\frac{u_{sd}-pL_q\\omega_{me}i_{sq}-R_si_{sd}}{L_d}$\n",
    "\n",
    "$\\frac{\\mathrm{d}\\epsilon_{el}}{\\mathrm{d}t} = p\\omega_{me}$\n",
    "</h3>\n",
    "The target for the agent is now to learn to control the currents freely. For this, a reference generator generates a trajectory that the agent has to follow. Therefore, he has to learn to solve the ODEs and convert voltages accordingly. \n",
    "\n",
    "For a deeper understanding of the used models behind the environment the documentation can be found [here](https://upb-lea.github.io/gym-electric-motor/).\n",
    "\n",
    "The following code is using our pre-written function set_env to import our pre-defined GEM environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pascal/miniconda3/envs/tf_5_env/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from setting_environment import set_env\n",
    "# creating the described environment\n",
    "env = set_env(training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Tensorforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "To take advantage of some already implemented deep-RL agents, we use the *tensorforce-framework*. It is built on *TensorFlow* and offers agents based on deep Q-networks, policy gradients, or actor-critic algorithms. \n",
    "\n",
    "For more information to specific agents or different modules that can be used, some good explanations can be found in the corresponding [documentation](https://tensorforce.readthedocs.io/en/latest/).\n",
    "   \n",
    "For the control task with a discrete action-space we will use a *deep-Q-network* (DQN). The necessary knowledge about DQN's can be acquired in the original article [here](https://www.nature.com/articles/nature14236) or for a deeper look into the basics of reinforcement learning in the book \"Reinforcement Learning: An Introduction\" by Richard S. Sutton and Andrew G. Barto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 Defining an Tensorforce-Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using tensorforce, you need to define a *tensorforce-environment*. This is done simply by using the **Environment.create** interface. This takes your normal gym-like environment with some additional arguments. Gym-like means in this context an environment, which is built after the example from the gym-environments from [Open-AI](https://github.com/openai/gym)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorforce.environments import Environment\n",
    "# maximal timesteps per episode for training \n",
    "max_episode_steps = 10000\n",
    "# creating tensorforce environment\n",
    "tf_env = Environment.create(environment=env,\n",
    "                            max_episode_timesteps=max_episode_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 Setting-up an Tensorforce-Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agent is created just like the environment. The agent's parameters can be passed as arguments from the create function or via a configuration as a dictionary or as *.json* file.\n",
    "In the following, the way via a dictionary is used.\n",
    "\n",
    "With the *tensorforce-framework* it is possible to define own network-architectures like it is shown in the code snippet. For some parameters, it can be useful to haven a decaying value during the training. A possible way for this is also shown in the following code.\n",
    "\n",
    "The exact meaning of the used parameters can be found in the already mentioned tensorforce documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a parameter decay for the exploration\n",
    "epsilon_decay = {'type': 'decaying',\n",
    "                 'decay': 'polynomial',\n",
    "                 'decay_steps': 50000,\n",
    "                 'unit': 'timesteps',\n",
    "                 'initial_value': 1.0,\n",
    "                 'decay_rate': 5e-2,\n",
    "                 'final_value': 5e-2,\n",
    "                 'power': 3.0}\n",
    "\n",
    "# defining a simple network architecture: 2 dense-layers with 64 nodes each\n",
    "net = [\n",
    "    dict(type='dense', size=64, activation='relu'),\n",
    "    dict(type='dense', size=64, activation='relu'),\n",
    "]\n",
    "\n",
    "# defining the parameters of an dqn-agent\n",
    "agent_config = {\n",
    "    'agent': 'dqn',\n",
    "    'memory': 200000,\n",
    "    'batch_size': 25,\n",
    "    'network': net,\n",
    "    'update_frequency': 1,\n",
    "    'start_updating': 10000,\n",
    "    'learning_rate': 1e-4,\n",
    "    'discount': 0.99,\n",
    "    'exploration': epsilon_decay,\n",
    "    'target_sync_frequency': 1000,\n",
    "    'target_update_weight': 1.0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorforce.agents import Agent\n",
    "\n",
    "tau = 1e-5\n",
    "simulation_time = 5 # seconds\n",
    "training_steps = int(simulation_time // tau)\n",
    "# creating agent via dictionary\n",
    "dqn_agent = Agent.create(agent=agent_config, environment=tf_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the agent is executed with the **tensorforce-runner**. The runner stores metrics during the training, like the reward per episode, and can be used to save learned agents. If you just want to experiment a little with an already trained agent, it is possible to skip the next cells and just load a pre-trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce.execution import Runner\n",
    "\n",
    "# create and train the agent\n",
    "runner = Runner(agent=dqn_agent, environment=tf_env)\n",
    "runner.run(num_timesteps=training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With accessing saved metrics from the runner, it for example possible to have a look on the mean-reward per episode or the corresponding episode-length. It is possible to load an array with the mean rewards per episode, so you don't have to run the whole training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to load the array\n",
    "rewards = np.load('saved_agents/rewards_per_episode.npy')\n",
    "episode_length = np.load('saved_agents/length_per_episode.npy')\n",
    "\n",
    "# comment, if the array is loaded above\n",
    "# accesing the metrics from runner\n",
    "#rewards = np.asarray(runner.episode_rewards)\n",
    "#episode_length = np.asarray(runner.episode_timesteps)\n",
    "\n",
    "# calculating the mean-reward per episode\n",
    "mean_reward = rewards/episode_length\n",
    "num_episodes = len(mean_reward)\n",
    "\n",
    "# plotting mean-reward over episodes\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(20,10))\n",
    "ax1.plot(range(num_episodes), mean_reward, linewidth=3)\n",
    "#plt.xticks(fontsize=15)\n",
    "ax1.set_ylabel('mean-reward', fontsize=22)\n",
    "ax1.grid(True)\n",
    "ax1.tick_params(axis=\"y\", labelsize=15) \n",
    "# plotting episode length over episodes\n",
    "ax2.plot(range(num_episodes), episode_length, linewidth=3)\n",
    "ax2.set_xlabel('# episodes', fontsize=22)\n",
    "ax2.set_ylabel('episode-length', fontsize=22)\n",
    "ax2.tick_params(axis=\"y\", labelsize=15) \n",
    "ax2.tick_params(axis=\"x\", labelsize=15) \n",
    "\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('number of episodes during training: ', len(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the agents trained model, makes it avaible for a seperate evalutation and further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the code and execute cell to save the trained agent\n",
    "# agent_path = 'saved_agents'\n",
    "# agent_name = 'dqn_agent_tensorforce'\n",
    "# runner.agent.save(directory=agent_path, filename=agent_name)\n",
    "# print('\\n agent saved \\n')\n",
    "# runner.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating the trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Loading a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a previously saved agent is available, it can be restored by using the runner to load the model with the **.load()** function. To load the agent it is necessary to pass the directory, the filename, the environment, and the agent configuration used for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce import Agent\n",
    "\n",
    "agent_path = 'saved_agents'\n",
    "agent_name = 'dqn_agent_tensorforce'\n",
    "\n",
    "dqn_agent = Agent.load(\n",
    "    directory=agent_path,\n",
    "    filename=agent_name,\n",
    "    environment=tf_env,\n",
    "    **agent_config\n",
    ")\n",
    "print('\\n agent loaded \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Evaluating the agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the trained agent as a controller, a typical loop to interact with the environment can be used, which is displayed in the cell below. Here it is necessary to define a new test-environment with the **set_env()** function, which disables the episode-time-limit for the agent.\n",
    "\n",
    "Now the agent takes the observations from the environment and reacts with an action, which is used to control the environment. To get an impression of how the trained agent performs, the trajectory of the control-states can be observed. A live-plot will be displayed in a jupyter-notebook. If you are using jupyter-lab, the following cell could cause problems regarding the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "env = set_env(time_limit = False, training=False)\n",
    "# currently the visualization crashes for larger values, than the defined value\n",
    "visualization_steps = int(9e4) \n",
    "obs = env.reset()\n",
    "for step in range(visualization_steps):\n",
    "    # getting the next action from the agent\n",
    "    actions = dqn_agent.act(obs, evaluation=True)\n",
    "    # the env return the next state, reward and the information, if the state is terminal\n",
    "    obs, reward, done, _ = env.step(action=actions)\n",
    "    # activating the visualization\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        # reseting the env, if a terminal state is reached\n",
    "        obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example a classic *environment-interaction loop* can be extended to access different metrics and values, e.g. the cumulated reward over all steps. The number of evaluation-steps can be reduced, but a higher variance of the evaluation result must than be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test agent\n",
    "env = set_env(time_limit = False, training=False)\n",
    "\n",
    "steps = 250000#int(1e5)\n",
    "\n",
    "rewards = []\n",
    "episode_lens = []\n",
    "\n",
    "obs = env.reset()\n",
    "terminal = False\n",
    "cumulated_rew = 0\n",
    "step_counter = 0\n",
    "episode_rew = 0\n",
    "\n",
    "for step in tqdm(range(steps)):\n",
    "    actions = dqn_agent.act(obs, evaluation=True)\n",
    "    obs, reward, done, _ = env.step(action=actions)\n",
    "    \n",
    "    cumulated_rew += reward\n",
    "    episode_rew += reward\n",
    "    \n",
    "    step_counter += 1\n",
    "\n",
    "    if done:\n",
    "        rewards.append(episode_rew)\n",
    "        episode_lens.append(step_counter)\n",
    "        episode_rew = 0\n",
    "        step_counter = 0\n",
    "        \n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "print(f' \\n Cumulated reward per step is {cum_rew/steps} \\n')\n",
    "print(f' \\n Number of episodes Reward {len(episode_lens)} \\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accesing the metrics from runner\n",
    "rewards = np.asarray(rewards)\n",
    "episode_length = np.asarray(episode_lens)\n",
    "# calculating the mean-reward per episode\n",
    "mean_reward = rewards/episode_length\n",
    "num_episodes = len(rewards)\n",
    "\n",
    "# plotting mean-reward over episodes\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(20,10))\n",
    "ax1.plot(range(num_episodes), mean_reward, linewidth=3)\n",
    "#plt.xticks(fontsize=15)\n",
    "ax1.set_ylabel('reward', fontsize=22)\n",
    "ax1.grid(True)\n",
    "ax1.tick_params(axis=\"y\", labelsize=15) \n",
    "# plotting episode length over episodes\n",
    "ax2.plot(range(num_episodes), episode_length, linewidth=3)\n",
    "ax2.set_xlabel('# episodes', fontsize=22)\n",
    "ax2.set_ylabel('episode-length', fontsize=22)\n",
    "ax2.tick_params(axis=\"y\", labelsize=15) \n",
    "ax2.tick_params(axis=\"x\", labelsize=15) \n",
    "\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('number of episodes during training: ', len(episode_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
