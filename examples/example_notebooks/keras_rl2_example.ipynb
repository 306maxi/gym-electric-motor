{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an RL agent with Keras-RL2 using a GEM environment\n",
    "\n",
    "This notebook serves as an educational introduction to the usage of Keras-RL2 using a GEM environment. Goal of this notebook is to give an understanding of what Keras-RL2 is and how to use it to train and evaluate a Reinforcement Learning agent which is able to solve a current control problem within the GEM toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can start you need to make sure that you have installed both, gym-electric-motor and Keras-rl2. You can install both easily using pip:\n",
    "\n",
    "- ```pip install gym-electric-motor```\n",
    "- ```pip install keras-rl2```\n",
    "\n",
    "Alternatively, you can install them and their latest developer version directly from GitHub (recommended) :\n",
    "\n",
    "- https://github.com/upb-lea/gym-electric-motor\n",
    "- https://github.com/wau/keras-rl2\n",
    "\n",
    "For this notebook, the following cell will do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/upb-lea/gym-electric-motor.git git+https://github.com/wau/keras-rl2.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up a GEM evironment\n",
    "\n",
    "The basic idea behind reinforcement learning is to create a so-called agent, that should learn by itself to solve a specified task in a given environment. \n",
    "This environment gives the agent feedback on its actions and reinforces the targeted behavior.\n",
    "In this notebook, the task is to train a controller for the current control of a *Permanent Magnet Synchronous Motor* (*PMSM*).\n",
    " \n",
    "In the following, the used GEM-environment is briefly presented, but this notebook does not focus directly on the detailed usage of GEM. If you are new to the used environment and interested in finding out what it does and how to use it, you should take a look at the [GEM cookbook](https://colab.research.google.com/github/upb-lea/gym-electric-motor/blob/master/examples/example_notebooks/GEM_cookbook.ipynb).\n",
    "\n",
    "To save some space in this notebook, there is a function defined in an external python file called **getting_environment.py**. If you want to know how the environment's parameters are defined you can take a look at that file. By simply calling the **get_env()** function from the external file, you can set up an environment for a *PMSM* with discrete inputs.\n",
    "\n",
    "The basic idea of the control setup from the GEM-environment is displayed in the following figure. \n",
    "\n",
    "![](../../docs/plots/SCML_Overview.svg)\n",
    "\n",
    "The agent controls the converter who converts the supply currents to the currents flowing into the motor - for the *PMSM*: $i_{sq}$ and $i_{sd}$\n",
    "\n",
    "In the continuous case, the agent's action equals a duty cycle which will be modulated into a corresponding voltage. \n",
    "\n",
    "In the discrete case, the agent's actions denote switching states of the converter at the given instant. Here, only a discrete amount of options are available. In this notebook, for the PMSM the *Discrete B6 Bridge Converter* with six switches is utilized per default. This converter provides a total of eight possible actions.\n",
    "\n",
    "![](../../docs/plots/B6.svg)\n",
    "\n",
    "The motor schematic is the following:\n",
    "\n",
    "\n",
    "![](../../docs/plots/ESBdq.svg)\n",
    "\n",
    "And the electrical ODEs for that motor are:\n",
    "\n",
    "<h3 align=\"center\">\n",
    "\n",
    "<!-- $\\frac{\\mathrm{d}i_{sq}}{\\mathrm{d}t} = \\frac{u_{sq}-pL_d\\omega_{me}i_{sd}-R_si_{sq}}{L_q}$\n",
    "\n",
    "$\\frac{\\mathrm{d}i_{sd}}{\\mathrm{d}t} = \\frac{u_{sd}-pL_q\\omega_{me}i_{sq}-R_si_{sd}}{L_d}$\n",
    "\n",
    "$\\frac{\\mathrm{d}\\epsilon_{el}}{\\mathrm{d}t} = p\\omega_{me}$\n",
    " -->\n",
    "\n",
    "   $ \\frac{\\mathrm{d}i_{sd}}{\\mathrm{d}t}=\\frac{u_{sd} + p\\omega_{me}L_q i_{sq} - R_s i_{sd}}{L_d} $ <br><br>\n",
    "    $\\frac{\\mathrm{d} i_{sq}}{\\mathrm{d} t}=\\frac{u_{sq} - p \\omega_{me} (L_d i_{sd} + \\mathit{\\Psi}_p) - R_s i_{sq}}{L_q}$ <br><br>\n",
    "   $\\frac{\\mathrm{d}\\epsilon_{el}}{\\mathrm{d}t} = p\\omega_{me}$\n",
    "\n",
    "</h3>\n",
    "\n",
    "The target for the agent is now to learn to control the currents. For this, a reference generator produces a trajectory that the agent has to follow. \n",
    "Therefore, it has to learn a function (policy) from given states, references and rewards to appropriate actions.\n",
    "\n",
    "For a deeper understanding of the used models behind the environment see the [documentation](https://upb-lea.github.io/gym-electric-motor/).\n",
    "Comprehensive learning material to RL is also [freely available](https://github.com/upb-lea/reinforcement_learning_course_materials).\n",
    "\n",
    "The following code is using the pre-written function ```get_env()``` to import a pre-defined GEM environment. \n",
    "Additionally, a ```RewardLogger``` callback is added to visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getting_environment import get_env\n",
    "env = get_env(training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train an Agent with Keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras-rl2](https://github.com/wau/keras-rl2) is a widely used Python library for reinforcement learning. It compiles a collection of Reinforcement Learning algorithm implementations.\n",
    "Most of the algorithms and their extensions are readily available and are quite stable. The ease of adopting to various environments adds to its popularity.\n",
    "For currently available RL algorithms see their [documentation](https://keras-rl.readthedocs.io/en/latest/agents/overview/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Imports\n",
    "The environment in this control problem poses a discrete action space. Therefore, the [Deep Q-Network (DQN)](https://arxiv.org/abs/1312.5602) is a suitable agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from getting_environment import get_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Parameters and Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DQN algorithm, a set of parameters has to be pre-defined.\n",
    "In particular, a multilayer perceptron (MLP) is used as function approximator within the DQN algorithm.\n",
    "Note: Although there are 8 possible actions, we ignore the last action as the voltage applied on the motor there is the same as for the first action. This effectively reduces the feature space that is to be explored and benefits training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters.\n",
    "gamma = 0.99\n",
    "time_limit = True\n",
    "buffer_size = 200000  # observation history size\n",
    "batch_size = 25  # mini batch size sampled from history at each update step\n",
    "env = get_env(time_limit, gamma, training=True)\n",
    "nb_actions = env.action_space.n\n",
    "window_length = 1\n",
    "\n",
    "# construct a MLP\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(window_length,) + env.observation_space.shape))\n",
    "model.add(Dense(64, activation='relu'))  # hidden layer 1\n",
    "model.add(Dense(64, activation='relu'))  # hidden layer 2\n",
    "model.add(Dense(nb_actions, activation='linear'))  # output layer\n",
    "\n",
    "# keras-rl2 objects\n",
    "memory = SequentialMemory(limit=200000, window_length=window_length)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(eps=0.2), 'eps', 1, 0.05, 0, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've setup the environment and defined your parameters starting the training is nothing more than a one-liner. For each algorithm all you have to do is call its ```fit()``` function.\n",
    "\n",
    "The advantage of keras-rl is that you can see all the important parameters and their values during training phase to get an intution of learning.\n",
    "Hence, there is no need of additional callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    policy=policy,\n",
    "    nb_actions=nb_actions,\n",
    "    memory=memory,\n",
    "    gamma=gamma,\n",
    "    batch_size=25,\n",
    "    train_interval=1,\n",
    "    memory_interval=1,\n",
    "    target_model_update=1000,\n",
    "    nb_steps_warmup=10000,\n",
    "    enable_double_dqn=True\n",
    ")\n",
    "\n",
    "dqn.compile(Adam(lr=1e-4),\n",
    "            metrics=['mse']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "history = dqn.fit(env,\n",
    "            nb_steps=500000,\n",
    "            action_repetition=1,\n",
    "            verbose=2,\n",
    "            visualize=True,\n",
    "            nb_max_episode_steps=10000,\n",
    "            log_interval=1000\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training is finished, you can save the model that your DQN-agent has learned for later reuse, e.g. for evaluation or continued training. For this purpose, ```save_weights()``` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = Path('saved_agents')\n",
    "weight_path.mkdir(parents=True, exist_ok=True)\n",
    "dqn.save_weights(str(weight_path / 'dqn_keras-RL2.hdf5'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras-rl2 agents have their own ```test()``` method which can be used to evaluate their performance.\n",
    "Here, the metric used is the total reward the agent accumulates at the end of each episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading a Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('saved_agents\\dqn_keras-RL2.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "env = get_env(time_limit = False, gamma=0.99, training=False)\n",
    "dqn.test(env,\n",
    "         nb_episodes=5,\n",
    "         nb_max_episode_steps=100000,\n",
    "         visualize=True\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
