{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an RL agent with Keras-RL2 using a GEM environment\n",
    "\n",
    "This notebook serves as an educational introduction to the usage of Keras-RL2 using a GEM environment. Goal of this notebook is to give an understanding what Keras-RL2 is and how to use it to train and evaluate a Reinforcement Learning agent which is able to solve a current control problem of the GEM toolbox.\n",
    "\n",
    "The following code snippets are only needed if you are executing this file directly from a cloned GitHub repository where you don't have GEM installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path().resolve().parent.parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can start you need to make sure that you have both gym-electric-motor and Keras-rl2 installed. You can install both easily using pip:\n",
    "\n",
    "- pip install gym-electric-motor\n",
    "- pip install keras-rl2\n",
    "\n",
    "Alternatively, you can install them and their latest developer version directly from GitHub:\n",
    "\n",
    "- https://github.com/upb-lea/gym-electric-motor\n",
    "\n",
    "You also need to make sure that numpy and gym are installed. You can install both using pip, too. After you have done that you should be able to execute the following cells without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up a GEM evironment\n",
    "\n",
    "This notebook does not focus directly on the usage of GEM and how to set up a GEM environment. If you are new to GEM and interested to find out what it does and how to use it we recommend taking a look at the educational notebook which is dealing with GEM.\n",
    "\n",
    "\n",
    "For this notebook, we will use a function defined in an external Python file called setting_environment.py. If you are interested to see how we defined our environment's parameters you can take a look at that file. We are using the Discrete Permanent Magnet Synchronous Motor Environment:\n",
    "\n",
    "- https://upb-lea.github.io/gym-electric-motor/parts/environments/pmsm_disc.html\n",
    "\n",
    "- https://upb-lea.github.io/gym-electric-motor/parts/physical_systems/electric_motors/pmsm.html\n",
    "\n",
    "The motor schematic is the following:\n",
    "\n",
    "![Motor Setup](img/ESBdq1.svg)\n",
    "\n",
    "And the electrical ODEs for that motor are:\n",
    "\n",
    "<h3 align=\"center\">\n",
    "\n",
    "$\\frac{\\mathrm{d}i_{sq}}{\\mathrm{d}t} = \\frac{u_{sq}-pL_d\\omega_{me}i_{sd}-R_si_{sq}}{L_q}$\n",
    "\n",
    "$\\frac{\\mathrm{d}i_{sd}}{\\mathrm{d}t} = \\frac{u_{sd}-pL_q\\omega_{me}i_{sq}-R_si_{sd}}{L_d}$\n",
    "\n",
    "$\\frac{\\mathrm{d}\\epsilon_{el}}{\\mathrm{d}t} = p\\omega_{me}$\n",
    "\n",
    "</h3>\n",
    "\n",
    "In GEM the agent controls the converter who converts the supply currents to the currents flowing into the motor - in our motor's case $i_{sq}$ and $i_{sd}$:\n",
    "\n",
    "![Motor Setup](img/SCML_Setting.svg)\n",
    "\n",
    "\n",
    "In the continuous case the agent's action is controlling the converter's output voltage directly through means of duty cycling. In the discrete case the agent's action decides which converter switches are open and which are closed. Therefore, only a discrete amount of options are available. For this environment we are using the Discrete B6 Bridge Converter which has three switches which amounts to a total of eight possible actions the agent can take:\n",
    "\n",
    "![Motor Setup](img/B6.svg)\n",
    "\n",
    "\n",
    "\n",
    "We wish for an agent to be able to control the currents freely. For this a reference generator generates a trajectory which the agent has to follow. Therefore, it has to internally learn to solve the ODEs and convert voltages accordingly. The following code is using our pre-written function set_env to import our pre-defined GEM environment. We are also using the callback RewardLogger to visualize our training progress after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setting_environment import set_env\n",
    "env = set_env(training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why Keras-RL2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras-rl2 is a collection of Reinforcement Learning algorithms implementation. It can be used in a scenario where you want to train an agent of a specific RL algorithm when you don't want to implement the algorithm yourself.\n",
    "\n",
    "Keras-rl2 is a widely used library for reinforcement learning. Most of the algorithms and their extensions are readily available and are quite stable.However, it is very vaguely implemented. But the ease of adopting to various environments makes it one of the widely used libraries.\n",
    "\n",
    "For currently available RL algorithms see their documentation:\n",
    "\n",
    "- https://keras-rl.readthedocs.io/en/latest/agents/overview/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an agent in Keras-rl2 you need two things: The agent and a function approximator representing the control policy. The agent is the algorithm you want to use to solve your problem. The policy defines the function estimation you want to use. Mostly supported are MLP and CNN function estimators. Check the algorithm in the documentation to see what estimator the algorithm you want to use supports. In the future, recurrent functions estimators are supposed to be implemented, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our control problem we have an environment with a discrete action space. Therefore, we decided for the Deep-Q-Network (DQN):\n",
    "\n",
    "- https://arxiv.org/abs/1312.5602\n",
    "\n",
    "For the implementation of the DQN you can check the following github repository:\n",
    "\n",
    "- https://github.com/keras-rl/keras-rl/blob/master/rl/agents/dqn.py#L89\n",
    "\n",
    "\n",
    "In this tutorial we use MLP. Necessary Imports like importing DQNAgent from keras-rl2, policy and memory has to be imported\n",
    "and it is important to Flatten observation, since Keras-rl doesn't support any other form of input. Currently, Feature engineering and invoking the environment are implemeneted in setting_environment.py. Hence it has to be imported too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from setting_environment import set_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Setting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DQN algorithm we have to define a set of values.These values will be directly used during compilation and training of an Agent\n",
    "\n",
    "Note: Although we have 8 possible actions, we consider only 7 of them since action 0 and action 7 are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters.\n",
    "gamma = 0.99\n",
    "time_limit = True\n",
    "buffer_size = 200000  # number of old obersation steps saved\n",
    "batch_size = 25  # mini batch size drawn at each update step\n",
    "env = set_env(time_limit, gamma, training=True)\n",
    "nb_actions = env.action_space.n\n",
    "window_length = 1\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(window_length,) + env.observation_space.shape))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "\n",
    "memory = SequentialMemory(limit=200000, window_length=window_length)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(eps=0.2), 'eps', 1, 0.05, 0, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Starting the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've setup the environment and defined your parameters starting the training is nothing more than a one-liner. For each algorithm all you have to do is call its .fit() function. The advantage of keras-rl is that you can see all the important parameters and their values during training phase to get an intution of learning. Hence we donot need to implement any callbacks seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    policy=policy,\n",
    "    nb_actions=nb_actions,\n",
    "    memory=memory,\n",
    "    gamma=gamma,\n",
    "    batch_size=25,\n",
    "    train_interval=1,\n",
    "    memory_interval=1,\n",
    "    target_model_update=1000,\n",
    "    nb_steps_warmup=10000,\n",
    "    enable_double_dqn=True\n",
    ")\n",
    "\n",
    "dqn.compile(Adam(lr=1e-4),\n",
    "            metrics=['mse']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "dqn.fit(env,\n",
    "        nb_steps=500000,\n",
    "        action_repetition=1,\n",
    "        verbose=2,\n",
    "        visualize=True,\n",
    "        nb_max_episode_steps=10000,\n",
    "        log_interval=1000\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training is finished, you can save the model that your DQNAgent has learned to reuse it later, e.g. for evaluation or if you want to continue your training. For this, we use \"____.save_weights\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('saved_agents\\dqn_keras-RL2.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have trained your agent you would like to see how well it does on your control problem. For this you can for example look at a visual representation of your currents in a training phase.\n",
    "in Testing phase, the widely used keras-rl2 has an edge since we have an option to control which values to be monitored throughout. Keras-rl DQN agent has its own \"__.test\" method which can be used to evaluate the performance of your agent \n",
    "\n",
    "Here the metric used is the total reward agent gets at the end of each episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, before we start our evaluation we have to load a trained agent or take the trained agent from the same code. To load an trained agent you only have to call the .load_weights(path) function of your algorithm with the respective path. \n",
    "\n",
    "If you are directly evaluating the performance(testing), note that the agent has to be compiled first(as shown in some cells above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('saved_agents\\dqn_keras-RL2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "env = set_env(time_limit = False, gamma=0.99, training=False)\n",
    "dqn.test(env,\n",
    "         nb_episodes=5,\n",
    "         nb_max_episode_steps=100000,\n",
    "         visualize=True\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
