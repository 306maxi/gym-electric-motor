{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a RL agent with Stable Baselines3 using a GEM environment\n",
    "\n",
    "This notebook serves as an educational introduction to the usage of Stable Baselines3 using a gym-electric-motor (GEM) environment. The goal of this notebook is to give an understanding of what Stable Baselines3 is and how to use it to train and evaluate a reinforcement learning agent that can solve a current control problem of the GEM toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can start you need to make sure that you have both gym-electric-motor and Stable-Baselines3 installed. You can install both easily using pip:\n",
    "\n",
    "- pip install gym-electric-motor\n",
    "- pip install stable-baselines3\n",
    "\n",
    "Alternatively, you can install them and their latest developer version directly from GitHub:\n",
    "\n",
    "- https://github.com/upb-lea/gym-electric-motor\n",
    "- https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "After you have done that you should be able to execute the following cells without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up a GEM evironment\n",
    "\n",
    "The basic idea behind reinforcement learning is to create a so-called agent, that should learn by itself to solve a specified task in a given environment. This environment gives the agent feedback on its actions an reinforces the targeted behavior.\n",
    "In this notebook, the task is to train a controller for the current control of a *Permanent Magnet Synchronous Motor* (*PMSM*).\n",
    " \n",
    "In the following, the used GEM-envrionment is briefly presented, but this notebook does not focus directly on the detailed usage of GEM. If you are new to the used environment and interested in finding out what it does and how to use it, you should take a look at the [educational notebook](GEM_tutorial.ipynb) which is dealing with GEM.\n",
    "\n",
    "To save some space in this notebook, there is a function defined in an external python file called **getting_environment.py**. If you want to know how the environment's parameters are defined you can take a look at that file. By simply calling the **get_env()** function from the external file, you can set up an environment for a *PMSM* with discrete inputs.\n",
    "\n",
    "The basic idea of the control-setup from the GEM-environment is displayed in the following figure. \n",
    "\n",
    "![Motor Setup](img/SCML_Setting.svg)\n",
    "\n",
    "The agent controls the converter who converts the supply currents to the currents flowing into the motor - for the *PMSM*: $i_{sq}$ and $i_{sd}$\n",
    "\n",
    "In the continuous case, the agent's action is controlling the converter's output voltage directly through means of duty cycling. In the discrete case, the agent's action decides which converter switches are open and which are closed. Therefore, only a discrete amount of options are available. This environment uses the *Discrete B6 Bridge Converter* which has three switches which leads to a total of eight possible actions the agent can take.\n",
    "\n",
    "![Motor Setup](img/B6.svg)\n",
    "\n",
    "The motor is described through the schematic:\n",
    "\n",
    "![Motor Setup](img/ESBdq1.svg)\n",
    "\n",
    "Which leads to the electrical ODEs:\n",
    "\n",
    "<h3 align=\"center\">\n",
    "\n",
    "$\\frac{\\mathrm{d}i_{sq}}{\\mathrm{d}t} = \\frac{u_{sq}-pL_d\\omega_{me}i_{sd}-R_si_{sq}}{L_q}$\n",
    "\n",
    "$\\frac{\\mathrm{d}i_{sd}}{\\mathrm{d}t} = \\frac{u_{sd}-pL_q\\omega_{me}i_{sq}-R_si_{sd}}{L_d}$\n",
    "\n",
    "$\\frac{\\mathrm{d}\\epsilon_{el}}{\\mathrm{d}t} = p\\omega_{me}$\n",
    "</h3>\n",
    "The target for the agent is now to learn to control the currents freely. For this, a reference generator generates a trajectory that the agent has to follow. Therefore, he has to learn to solve the ODEs and convert voltages accordingly. \n",
    "\n",
    "For a deeper understanding of the used models behind the environment the documentation can be found [here](https://upb-lea.github.io/gym-electric-motor/).\n",
    "\n",
    "The following code is using the pre-written function get_env to import a pre-defined GEM environment. Additionally, a RewardLogger callback is added to visualize the training progress after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_electric_motor.core import Callback\n",
    "class RewardLogger(Callback):\n",
    "    def __init__(self):\n",
    "        self._step_rewards = []\n",
    "        self._mean_episode_rewards = []\n",
    "\n",
    "    def on_step_end(self):\n",
    "        self._step_rewards.append(self._env._reward)\n",
    "    \n",
    "    def on_reset_begin(self):\n",
    "        self._mean_episode_rewards.append(np.mean(self._step_rewards))\n",
    "        self._step_rewards = []\n",
    "        \n",
    "    def on_close(self):\n",
    "        np.save(Path.cwd() / \"saved_agents\" / \"EpisodeRewards.npy\", np.array(self._mean_episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getting_environment import get_env\n",
    "env = get_env(training=True, callbacks=[RewardLogger()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is Stable Baselines3?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Baselines3 collects Reinforcement Learning algorithms implemented in Pytorch. You can use it when you want to train an agent of a specific RL algorithm if you don't want to implement the algorithm yourself.\n",
    "\n",
    "Stable Baselines3 is still a very new library with its current release being 0.9. That is why its collection of algorithms is not very large yet and most algorithms lack more advanced variants. However, its authors planned to broaden the available algorithms in the future. For currently available algorithms see their documentation:\n",
    "\n",
    "- https://stable-baselines3.readthedocs.io/en/master/guide/rl.html\n",
    "\n",
    "To use an agent provided by Stable Baselines3 your environment has to have a gym interface:\n",
    "\n",
    "- https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an agent in Stable Baselines3 you need two things: The agent's algorithm and a function approximator representing the control policy. Mostly supported are MLP and CNN function estimators. Check the algorithm in the documentation to see what estimator the algorithm you want to use supports. The authors current plan is to implement recurrent function estimators in the future, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment in this control problem has got a discrete action space. Therefore, the Deep-Q-Network (DQN) is a suitable agent:\n",
    "\n",
    "- https://arxiv.org/abs/1312.5602\n",
    "\n",
    "For the implementation of the DQN you can check Stable Baslines3's docs and see, that currently the MLP and the CNN are supported:\n",
    "\n",
    "- https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
    "\n",
    "In this tutorial a MLP is used. For this you have to import the DQN and the MlpPolicy. You can also see in the docs which gym spaces for the observation and the actions are supported. You might have to take that into account for your environment. For example you have to flatten the observation for GEM because Stable Baselines3 does not support tuples as observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn import MlpPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Setting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DQN algorithm you have to define a set of values. The policy_kwargs dictionary is a parameter which is directly given to the MlpPolicy. The net_arch key defines the network architecture of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 200000 #number of old obsersation steps saved\n",
    "learning_starts = 10000 # memory warmup\n",
    "train_freq = 1 # prediction network gets an update each train_freq's step\n",
    "batch_size = 25 # mini batch size drawn at each update step\n",
    "policy_kwargs = {\n",
    "        'net_arch': [64,64] # hidden layer size of MLP\n",
    "        }\n",
    "exploration_fraction = 0.1 # Fraction of training steps the epsilon decays \n",
    "target_update_interval = 1000 # Target network gets updated each target_update_interval's step\n",
    "gamma = 0.99\n",
    "verbose = 1 # verbosity of stable basline's prints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you have to define how long your agent will train. You can just set a concrete number of steps or use knowledge of the environment's temporal resolution to define an in-simulation training time. In this example the agent is trained for five seconds which translates in this environment's case to 500000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1e-5\n",
    "simulation_time = 5 # seconds\n",
    "nb_steps = int(simulation_time // tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Starting the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've setup the environment and defined your parameters starting the training is nothing more than a one-liner. For each algorithm all you have to do is call its .learn() function. However, you should note that the execution of the training can take a long time. Currently, Stable Baselines3 does not provide any means of reward observation during training. Therefore, a RewardLogger callback is used for this environment (see code a few cells above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = DQN(MlpPolicy, env, buffer_size=buffer_size, learning_starts=learning_starts ,train_freq=train_freq, \n",
    "            batch_size=batch_size, gamma=gamma, policy_kwargs=policy_kwargs, \n",
    "            exploration_fraction=exploration_fraction, target_update_interval=target_update_interval,\n",
    "            verbose=verbose)\n",
    "model.learn(total_timesteps=nb_steps)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training has finished you can save the model your DQN has learned to reuse it later, e.g. for evaluation or if you want to continue your training. For this, each Stable Baselines3 algorithm has a .save() function where you only have to specify your path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(Path.cwd() / \"saved_agents\" / \"TutorialAgent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have trained your agent you would like to see how well it does on your control problem. For this you can look at a visual representation of your currents in a test trajectory or see how well your agent does in a test scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, before you start your evaluation you have to load a trained agent or take the trained agent from above which is still saved in the variable model. To load a trained agent you simply have to call the .load() function of your algorithm with the respective path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(Path(__file__).parent / \"saved_agents\" / \"TutorialAgent\")  #your agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Taking a look at the mean reward per episode during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RewardLogger callback saved the mean reward per episode during training. There you can observe, how the training reward did grow over time and if any problems occured during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.load(Path.cwd() / \"saved_agents\" / \"EpisodeRewards.npy\")[1:] # your training rewards\n",
    "plt.grid(True)\n",
    "plt.xlim(0,len(rewards))\n",
    "plt.ylim(min(rewards), 0)\n",
    "plt.yticks(np.arange(min(rewards), 1, 1.0))\n",
    "plt.tick_params(axis = 'y', left = False, labelleft = False)\n",
    "plt.xticks(np.arange(0, len(rewards), 10))\n",
    "plt.xlabel('#Episode')\n",
    "plt.ylabel('Mean Reward per Episode (Qualitative)')\n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Taking a look at a test trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can to take a look at a test trajectory to see how well your trained agent is able to control the currents to follow the test trajectory. For the agent to decide for an action given an observation you can just call its .predict() function. The key deterministic is important so that the agent is not using a stochastic policy like epsilon greedy but is instead chosing an action greedily. The env.render() will then visualize the agent's and reference generator's trajectories as well as the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "env = get_env(time_limit = False, training=False)\n",
    "visualization_steps = int(9e4) # currently this crashes for larger values\n",
    "obs = env.reset()\n",
    "for i in range(visualization_steps):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Calculating further evaluation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the knowledge you acquired in the previous sections you are now able to train and evaluate any in Stable Baselines3 available reinforcement learning algorithm. The code below should give you an example how to use the trained agent to calculate a mean reward and mean episode length over a specific amount of steps. For further questions you can always have a look at the documentation of gym-electric-motor and Stable Baselines3 or raise an issue in their respective GitHub repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps = int(1e5) #1 milion for stability reasons\n",
    "cum_rew = 0\n",
    "episode_step = 0\n",
    "episode_lengths = []\n",
    "for i in range(test_steps):\n",
    "    print(f\"{i+1}\", end = '\\r')\n",
    "    episode_step += 1\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    cum_rew += reward\n",
    "    if done:\n",
    "        episode_lengths.append(episode_step)\n",
    "        episode_step = 0\n",
    "        obs = env.reset()\n",
    "print(f\"The reward per step with {test_steps} steps was: {cum_rew/test_steps:.4f} \")\n",
    "print(f\"The average Episode length was: {round(np.mean(episode_lengths))} \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
