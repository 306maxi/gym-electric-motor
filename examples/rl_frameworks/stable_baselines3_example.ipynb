{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a RL agent with Stable Baselines3 using a GEM environment\n",
    "\n",
    "This notebook serves as an educational introduction to the usage of stable-baselines3 using a GEM environment. Goal of this notebook is to give an understanding what stable-baselines3 is and how to use it to train and evaluate an Reinforcement Learning agent, which is able to solve a current control problem of the GEM toolbox.\n",
    "\n",
    "The following code snippets are only needed if you are executing this file directly from a cloned GitHub repository where you don't have GEM installed directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path().resolve().parent.parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can start you need to make sure that you have both gym-electric-motor and Stable-Baselines3 installed. You can install both easily using pip:\n",
    "\n",
    "- pip install gym-electric-motor\n",
    "- pip install stable-baselines3\n",
    "\n",
    "Alternatively, you can install them and their latest developer version directly from GitHub:\n",
    "\n",
    "- https://github.com/upb-lea/gym-electric-motor\n",
    "- https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "You also need to make sure that numpy and gym are installed. You can install both using pip, too. After you have done that you should be able to execute the following cells without any problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up a GEM evironment\n",
    "\n",
    "This notebook does not focus directly on the usage of GEM and how to set up a GEM environment. If you are new to GEM and interested to find out what it does and how to use it we recommend taking a look at the educational notebook which is dealing with GEM.\n",
    "\n",
    "\n",
    "For this notebook, will use a function defined in an external Python file called setting_environment.py. If you are interested to see, how we defined our environment's parameters you can take a look into that file. We are using the Discrete DC Permanently Excited Motor Environment:\n",
    "\n",
    "- https://upb-lea.github.io/gym-electric-motor/parts/environments/dc_permex_disc.html\n",
    "\n",
    "- https://upb-lea.github.io/gym-electric-motor/parts/physical_systems/electric_motors/pmsm.html\n",
    "\n",
    "The motor schematic is the following:\n",
    "\n",
    "![Motor Setup](img/ESBdq1.svg)\n",
    "\n",
    "And the electrical ODEs of that motor are:\n",
    "\n",
    "<h3 align=\"center\">\n",
    "\n",
    "$\\frac{\\mathrm{d}i_{sq}}{\\mathrm{d}t} = \\frac{u_{sq}-pL_d\\omega_{me}i_{sd}-R_si_{sq}}{L_q}$\n",
    "\n",
    "$\\frac{\\mathrm{d}i_{sd}}{\\mathrm{d}t} = \\frac{u_{sd}-pL_q\\omega_{me}i_{sq}-R_si_{sd}}{L_d}$\n",
    "\n",
    "$\\frac{\\mathrm{d}\\epsilon_{el}}{\\mathrm{d}t} = p\\omega_{me}$\n",
    "\n",
    "</h3>\n",
    "\n",
    "In the end we wish for an agent which is able to solve the current control problem of this environment. This means it should control the system such that $i_{sq}$ and $i_{sd}$ follow a given trajectory. The following code is using our pre-written function set_env to import our pre-defined GEM environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setting_environment import set_env\n",
    "env = set_env(training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is Stable Baselines3?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable-Baselines3 is a collection of Reinforcement Learning algorithms implemented in Pytorch. It can be used in a scenario where you want to train an agent of a specific RL algorithm if you don't want to implement the algorithm yourself.\n",
    "\n",
    "Stable Baselines3 is still a very new library with it's current release being 0.9. That is why its selection of algorithms is not very large yet and most algorithms lack more sophisticated variants. However, it is planned for the future to broaden the available algorithms. For the currently available algorithms see their documentation:\n",
    "\n",
    "- https://stable-baselines3.readthedocs.io/en/master/guide/rl.html\n",
    "\n",
    "To use an agent provided by Stable Baselines3 your environment has to have a gym interface:\n",
    "\n",
    "- https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an agent in Stable Baselines3 you need two things: The agent and a policy. The agent is the algorithm you want to use to solve your problem. The policy defines the function estimation you want to use. Mostly supported are MLP and CNN policies for the respective neural network architecture. Check the algorithm in the documentation to see what policy the algorithm you want to use supports. In the future recurrent policies are supposed to be implemented, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our control problem we have an environment with a discrete action space. Therefore, we decided for the Deep-Q-Network (DQN):\n",
    "\n",
    "- https://arxiv.org/abs/1312.5602\n",
    "\n",
    "For the implementation of the DQN you can check Stable Baslines3's docs and see, that currently the MLP and the CNN policy are supported:\n",
    "\n",
    "- https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
    "\n",
    "In our case only the MLP policy does make sense. That is why we have to import the DQN and the MlpPolicy. You can also see which gym spaces for the observation and the actions are supported. You might have to take that into account for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn import MlpPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Setting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DQN algorithm we have to define a set of values. The policy_kwargs dictionary is a parameter which is directly given to the MlpPolicy. The net_arch key defines the network architecture of our MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 200000 #number of old obersation steps saved\n",
    "learning_starts = 10000 # memory warmup\n",
    "train_freq = 1 # prediction network gets an update each train_freq's step\n",
    "batch_size = 25 # mini batch size drawn at each update step\n",
    "policy_kwargs = {\n",
    "        'net_arch': [64,64] # hidden layer size of MLP\n",
    "        }\n",
    "exploration_fraction = 0.1 # Fraction of training steps the epsilon decays \n",
    "target_update_interval = 1000 # Target network gets updated each target_update_interval's step\n",
    "verbose = 1 # verbosity of stable basline's prints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we have to define how long our agent will train. We can just set a concrete number of steps or use our knowledge of the environment's temporal resolution to define an in-simulation training time. In this example we want to train the agent for 5 seconds which will translate to 500000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1e-5\n",
    "simulation_time = 5 # seconds\n",
    "nb_steps = int(simulation_time // tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Starting the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've setup the environment and defined your parameters starting the training is nothing more than an one-liner. For each algorithm all you have to do is call its .learn() function. However, you should note that the execution of the training can take a long time. Don't execute the next line if you don't have that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(MlpPolicy, env, buffer_size=buffer_size, learning_starts=learning_starts ,train_freq=train_freq, \n",
    "            batch_size=batch_size, gamma=gamma, policy_kwargs=policy_kwargs, \n",
    "            exploration_fraction=exploration_fraction, target_update_interval=target_update_interval,\n",
    "            verbose=verbose)\n",
    "model.learn(total_timesteps=nb_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training has finished you can save the model your DQN has learned to reuse it later, e.g. for evaluation or if you want to continue your training. For this, each Stable Baselines3 algorithm has a .save() function where you only have to specify your path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(Path(__file__).parent / \"saved_agents\" / \"TutorialAgent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have trained your agent you would like to see how well it does on your control problem. For this you can for example look at a visual representation of your currents in a test trajectory or see how well the reward of of your agent is in a test scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, before we start our evaluation let us load a pre-trained agent. If you have executed the provided code above you can either uncomment the next line of code or try to load your own saved agent. To load an trained agent you only have to call the .load() function of your algorithm with the respective path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(Path(__file__).parent / \"saved_agents\" / \"TutorialPreTrainedAgent\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Taking a look at a test trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we want to take a look at a test trajectory and see how well the trained agent is able to control the currents to behave like the test trajectory. For the agent to take an action given an observation you can just call its .predict() function. The key deterministic is important so that the agent is not using a stochastic policy like epsilon greedy but is just chosing an action greedily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = set_env(training=False)\n",
    "visualization_steps = int(9e4) # currently this crashes for larger values\n",
    "obs = env.reset()\n",
    "for i in range(visualization_steps):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    cum_rew_episode += reward\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Calculating further evaluation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the knowledge you acquired in the previous sections you are now able to train and evaluate any in Stable Baselines3 available Reinforcement Learning algorithm. The code below should give you an example how to use the trained agent to calculate a mean reward and mean episode length over a specific amount of steps. For further questions you can always have a look at the documentation of gym-electric-motor and Stable Baselines3 or raise an issue in their respective GitHub repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps = int(1e6) #1 milion for stability reasons\n",
    "cum_rew = 0\n",
    "episode_step = 0\n",
    "for i in range(test_steps):\n",
    "    print(f\"{i+1}\", end = '\\r')\n",
    "    episode_step += 1\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    cum_rew += reward\n",
    "    if done:\n",
    "        episode_lengths.append(episode_step)\n",
    "        episode_step = 0\n",
    "        obs = env.reset()\n",
    "print(f\"The reward per step with {test_steps} steps was: {cum_rew_testing_period/test_steps:.4f} \")\n",
    "print(f\"The average Episode length was: {round(np.mean(episode_lengths))} \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
