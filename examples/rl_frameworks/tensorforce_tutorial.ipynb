{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorforce Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"insert introduction for notebook\"\n",
    "\n",
    "    - link to git repo/ docu\n",
    "    - explanation to framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pascal/miniconda3/envs/tf_5_env/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorforce.environments import Environment\n",
    "from tensorforce.agents import Agent\n",
    "from tensorforce.execution import Runner\n",
    "\n",
    "from setting_environment import set_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics in Deep-Reinforcement-Learning (or The very basic in Deep-Reinforcement-Learning)\n",
    "    - why deep learning/ why/use (of) neural net \n",
    "    - intuition behind q-learning, dqn's\n",
    "    \n",
    "    \n",
    "    - literature references: paper from deepmind (dqn), barto-sutton book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining GEM-Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - explain: setup, environment, parameters, refer to gem-notebook example\n",
    "    - link to docu, github\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eps_steps = 10000\n",
    "simulation_steps = 5000\n",
    "gem_env = set_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Tensorforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Defining an Tensorforce-Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"describing definiton and usage of env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tensorforce environment\n",
    "tensor_env = Environment.create(environment=gem_env,\n",
    "                                max_episode_timesteps=max_eps_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Setting-up an Tensorforce-Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" explaining different agents, for dqn agent hpyerparameter or other possible \n",
    "parameter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameters for dqn-agent\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "hyperparameter for agent_config:\n",
    "\n",
    "    memory: size of replay-buffer\n",
    "    batch_size: size of mini-batch used for training\n",
    "    network: net-architect for dqn\n",
    "    update_frequency: Frequency of updates\n",
    "    start_updating: memory warm-up steps\n",
    "    learning_rate for optimizer\n",
    "    discount: gamma/ discount of future rewards\n",
    "    target_sync_frequency: Target network gets updated 'sync_freq' steps\n",
    "    target_update_weight: weight for target-network update\n",
    "\n",
    "\"\"\"\n",
    "epsilon_decay = {'type': 'decaying',\n",
    "                 'decay': 'polynomial',\n",
    "                 'decay_steps': 50000,\n",
    "                 'unit': 'timesteps',\n",
    "                 'initial_value': 1.0,\n",
    "                 'decay_rate': 5e-2,\n",
    "                 'final_value': 5e-2,\n",
    "                 'power': 3.0}\n",
    "net = [\n",
    "    dict(type='dense', size=64, activation='relu'),\n",
    "    dict(type='dense', size=64, activation='relu'),\n",
    "    #dict(type='linear', size=7)\n",
    "]\n",
    "\n",
    "agent_config = {\n",
    "    'agent': 'dqn',\n",
    "    'memory': 200000,\n",
    "    'batch_size': 25,\n",
    "    'network': net,\n",
    "    'update_frequency': 1,\n",
    "    'start_updating': 10000,\n",
    "    'learning_rate': 1e-4,\n",
    "    'discount': 0.99,\n",
    "    'exploration': epsilon_decay,\n",
    "    'target_sync_frequency': 1000,\n",
    "    'target_update_weight': 1.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pascal/miniconda3/envs/tf_5_env/lib/python3.7/site-packages/tensorforce/core/module.py:701: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# creating the agent\n",
    "dqn_agent = Agent.create(agent=agent_config, environment=tensor_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"explaining parameters, usage, possibilities to save and load models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Timesteps:  95%|█████████▌| 4770/5000 [00:06<00:00, 769.80it/s, mean_reward=n/a]"
     ]
    }
   ],
   "source": [
    "# creating tensorforce-runner to train the agent\n",
    "runner = Runner(agent=dqn_agent, \n",
    "                environment=tensor_env, \n",
    "                evaluation=True)\n",
    "runner.run(num_timesteps=simulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'environments': [<tensorforce.environments.environment.EnvironmentWrapper at 0x7faaadfca850>],\n",
       " 'is_environment_external': True,\n",
       " 'is_environment_remote': False,\n",
       " 'evaluation': True,\n",
       " 'is_agent_external': True,\n",
       " 'agent': <tensorforce.agents.dqn.DeepQNetwork at 0x7faaadfcadd0>,\n",
       " 'num_episodes': inf,\n",
       " 'num_timesteps': 5000,\n",
       " 'num_updates': inf,\n",
       " 'batch_agent_calls': False,\n",
       " 'sync_timesteps': False,\n",
       " 'sync_episodes': False,\n",
       " 'num_sleep_secs': 0.001,\n",
       " 'callback_episode_frequency': 1,\n",
       " 'callback_timestep_frequency': inf,\n",
       " 'callback': <function tensorforce.execution.runner.Runner.run.<locals>.tqdm_callback(runner, parallel)>,\n",
       " 'episode_rewards': [-259.1547722954568,\n",
       "  -279.1254744748554,\n",
       "  -208.3181660885444,\n",
       "  -710.8954496274285,\n",
       "  -199.99999999999983,\n",
       "  -251.05953297777995,\n",
       "  -211.5264861214116,\n",
       "  -203.30936999330999,\n",
       "  -274.1220545196185,\n",
       "  -264.46374036432144,\n",
       "  -223.63027048437854,\n",
       "  -342.3466699181473,\n",
       "  -209.8173267184947,\n",
       "  -216.35810134116537,\n",
       "  -256.8655278678994,\n",
       "  -289.1566321016218,\n",
       "  -502.31944220072324,\n",
       "  -232.306133113591,\n",
       "  -274.5688545310042,\n",
       "  -208.73393493110828,\n",
       "  -298.9366747095405,\n",
       "  -279.95471483508425,\n",
       "  -252.8994309343888,\n",
       "  -220.09138133650947,\n",
       "  -213.3779138560496,\n",
       "  -361.64416033189605],\n",
       " 'episode_timesteps': [178,\n",
       "  125,\n",
       "  13,\n",
       "  1944,\n",
       "  1,\n",
       "  82,\n",
       "  18,\n",
       "  5,\n",
       "  224,\n",
       "  197,\n",
       "  43,\n",
       "  178,\n",
       "  15,\n",
       "  23,\n",
       "  75,\n",
       "  147,\n",
       "  509,\n",
       "  46,\n",
       "  295,\n",
       "  13,\n",
       "  243,\n",
       "  280,\n",
       "  85,\n",
       "  31,\n",
       "  16,\n",
       "  373],\n",
       " 'episode_seconds': [0.4517648220062256,\n",
       "  0.1629035472869873,\n",
       "  0.020861148834228516,\n",
       "  2.268855571746826,\n",
       "  0.003542661666870117,\n",
       "  0.11411476135253906,\n",
       "  0.026752233505249023,\n",
       "  0.00957179069519043,\n",
       "  0.2761685848236084,\n",
       "  0.22784948348999023,\n",
       "  0.054019927978515625,\n",
       "  0.20795941352844238,\n",
       "  0.019433259963989258,\n",
       "  0.035233497619628906,\n",
       "  0.08793497085571289,\n",
       "  0.1847987174987793,\n",
       "  0.614769697189331,\n",
       "  0.0662226676940918,\n",
       "  0.35011792182922363,\n",
       "  0.018323898315429688,\n",
       "  0.31895971298217773,\n",
       "  0.3644547462463379,\n",
       "  0.1109166145324707,\n",
       "  0.04380464553833008,\n",
       "  0.02367401123046875,\n",
       "  0.4708225727081299],\n",
       " 'episode_agent_seconds': [0.38436079025268555,\n",
       "  0.12490320205688477,\n",
       "  0.014785289764404297,\n",
       "  1.7733068466186523,\n",
       "  0.0012786388397216797,\n",
       "  0.08748674392700195,\n",
       "  0.019412755966186523,\n",
       "  0.005824089050292969,\n",
       "  0.21270465850830078,\n",
       "  0.17635011672973633,\n",
       "  0.0407559871673584,\n",
       "  0.16216182708740234,\n",
       "  0.013864517211914062,\n",
       "  0.027314186096191406,\n",
       "  0.06758761405944824,\n",
       "  0.14258503913879395,\n",
       "  0.47548961639404297,\n",
       "  0.05134439468383789,\n",
       "  0.27077412605285645,\n",
       "  0.012700796127319336,\n",
       "  0.24773049354553223,\n",
       "  0.2819068431854248,\n",
       "  0.08371806144714355,\n",
       "  0.03249526023864746,\n",
       "  0.016974925994873047,\n",
       "  0.36890339851379395],\n",
       " 'evaluation_rewards': [-259.1547722954568,\n",
       "  -279.1254744748554,\n",
       "  -208.3181660885444,\n",
       "  -710.8954496274285,\n",
       "  -199.99999999999983,\n",
       "  -251.05953297777995,\n",
       "  -211.5264861214116,\n",
       "  -203.30936999330999,\n",
       "  -274.1220545196185,\n",
       "  -264.46374036432144,\n",
       "  -223.63027048437854,\n",
       "  -342.3466699181473,\n",
       "  -209.8173267184947,\n",
       "  -216.35810134116537,\n",
       "  -256.8655278678994,\n",
       "  -289.1566321016218,\n",
       "  -502.31944220072324,\n",
       "  -232.306133113591,\n",
       "  -274.5688545310042,\n",
       "  -208.73393493110828,\n",
       "  -298.9366747095405,\n",
       "  -279.95471483508425,\n",
       "  -252.8994309343888,\n",
       "  -220.09138133650947,\n",
       "  -213.3779138560496,\n",
       "  -361.64416033189605],\n",
       " 'evaluation_timesteps': [178,\n",
       "  125,\n",
       "  13,\n",
       "  1944,\n",
       "  1,\n",
       "  82,\n",
       "  18,\n",
       "  5,\n",
       "  224,\n",
       "  197,\n",
       "  43,\n",
       "  178,\n",
       "  15,\n",
       "  23,\n",
       "  75,\n",
       "  147,\n",
       "  509,\n",
       "  46,\n",
       "  295,\n",
       "  13,\n",
       "  243,\n",
       "  280,\n",
       "  85,\n",
       "  31,\n",
       "  16,\n",
       "  373],\n",
       " 'evaluation_seconds': [0.4517648220062256,\n",
       "  0.1629035472869873,\n",
       "  0.020861148834228516,\n",
       "  2.268855571746826,\n",
       "  0.003542661666870117,\n",
       "  0.11411476135253906,\n",
       "  0.026752233505249023,\n",
       "  0.00957179069519043,\n",
       "  0.2761685848236084,\n",
       "  0.22784948348999023,\n",
       "  0.054019927978515625,\n",
       "  0.20795941352844238,\n",
       "  0.019433259963989258,\n",
       "  0.035233497619628906,\n",
       "  0.08793497085571289,\n",
       "  0.1847987174987793,\n",
       "  0.614769697189331,\n",
       "  0.0662226676940918,\n",
       "  0.35011792182922363,\n",
       "  0.018323898315429688,\n",
       "  0.31895971298217773,\n",
       "  0.3644547462463379,\n",
       "  0.1109166145324707,\n",
       "  0.04380464553833008,\n",
       "  0.02367401123046875,\n",
       "  0.4708225727081299],\n",
       " 'evaluation_agent_seconds': [0.38436079025268555,\n",
       "  0.12490320205688477,\n",
       "  0.014785289764404297,\n",
       "  1.7733068466186523,\n",
       "  0.0012786388397216797,\n",
       "  0.08748674392700195,\n",
       "  0.019412755966186523,\n",
       "  0.005824089050292969,\n",
       "  0.21270465850830078,\n",
       "  0.17635011672973633,\n",
       "  0.0407559871673584,\n",
       "  0.16216182708740234,\n",
       "  0.013864517211914062,\n",
       "  0.027314186096191406,\n",
       "  0.06758761405944824,\n",
       "  0.14258503913879395,\n",
       "  0.47548961639404297,\n",
       "  0.05134439468383789,\n",
       "  0.27077412605285645,\n",
       "  0.012700796127319336,\n",
       "  0.24773049354553223,\n",
       "  0.2819068431854248,\n",
       "  0.08371806144714355,\n",
       "  0.03249526023864746,\n",
       "  0.016974925994873047,\n",
       "  0.36890339851379395],\n",
       " 'timesteps': 5159,\n",
       " 'episodes': 26,\n",
       " 'updates': 0,\n",
       " 'tqdm': Timesteps:  96%|█████████▌| 4786/5000 [00:23<00:00, 769.80it/s, mean_reward=n/a],\n",
       " 'tqdm_last_update': 4786,\n",
       " 'evaluation_run': True,\n",
       " 'save_best_agent': None,\n",
       " 'evaluation_callback': <function tensorforce.execution.runner.Runner.run.<locals>.<lambda>(r)>,\n",
       " 'episode_reward': [0.0],\n",
       " 'episode_timestep': [0],\n",
       " 'episode_agent_second': [],\n",
       " 'episode_start': [],\n",
       " 'evaluation_agent_second': 0.0,\n",
       " 'evaluation_start': 1600021532.1873355,\n",
       " 'terminate': 2,\n",
       " 'prev_terminals': [1],\n",
       " 'states': [array([-0.36317104,  0.17704685,  0.20016074,  0.94754815, -1.14770889,\n",
       "          1.2165041 , -0.15345873,  1.        ,  1.        , -1.        ,\n",
       "          1.17950106, -0.62173557,  0.04304377,  0.99907319,  1.        ,\n",
       "          0.02519529, -0.02630133])],\n",
       " 'terminals': [1],\n",
       " 'rewards': [-199.99999999999983],\n",
       " 'evaluation_internals': OrderedDict()}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating the trained Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" explaining evaluation loop and metrics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"visualizing metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
