{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym_electric_motor as gem\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from gym.wrappers import FlattenObservation\n",
    "#from tf_agents.environments.wrappers import FlattenObservationsWrapper\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from gym_electric_motor.visualization import MotorDashboard, ConsolePrinter\n",
    "from gym_electric_motor.physical_systems import ConstantSpeedLoad\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_generator = gem.reference_generators.WienerProcessReferenceGenerator(reference_state='i_sq')\n",
    "d_generator = gem.reference_generators.WienerProcessReferenceGenerator(reference_state='i_sd')\n",
    "rg = gem.reference_generators.MultipleReferenceGenerator([q_generator, d_generator])\n",
    "\n",
    "# Change of the default motor parameters.\n",
    "motor_parameter = dict(\n",
    "    r_s=15e-3, l_d=0.37e-3, l_q=1.2e-3, psi_p=65.6e-3, p=3, j_rotor=0.06\n",
    ")\n",
    "limit_values = dict(\n",
    "    i=160*1.41,\n",
    "    omega=12000 * np.pi / 30,\n",
    "    u=450\n",
    ")\n",
    "nominal_values = {key: 0.7 * limit for key, limit in limit_values.items()}\n",
    "u_sup = 400\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "\n",
    "gym_env_kwargs1 = {'visualization': MotorDashboard(plots = ['i_sq', 'i_sd', 'reward']),\n",
    "            # parameterize the PMSM\n",
    "               'motor_parameter' : motor_parameter,\n",
    "               'limit_values' : limit_values,\n",
    "               'nominal_values' : nominal_values,\n",
    "               'u_sup' : u_sup,\n",
    "               'load' : ConstantSpeedLoad(omega_fixed=1000 * np.pi / 30),\n",
    "               #'state_filter' :['i_sq', 'i_sd', 'epsilon'],  # todo\n",
    "               #'reward_weights' : {'i_sq': 1000, 'i_sd': 1000},\n",
    "               #'reward_power' : 0.5,\n",
    "               #'observed_states' : ['i_sq', 'i_sd'],\n",
    "           \n",
    "            'tau' : 1e-5 ,\n",
    "                  \n",
    "                  ## pass a reward function with a gamma!!  todo\n",
    "            # turn off terminations via limit violation and parameterize the reward function\n",
    "            'reward_function' : gem.reward_functions.WeightedSumOfErrors(observed_states=['i_sq', 'i_sd'], \n",
    "                                                                        reward_weights={'i_sq': 1, 'i_sd': 1},\n",
    "                                                                        #constraint_monitor = SqdCurrentMonitor(),\n",
    "                                                                        gamma = gamma,\n",
    "                                                                        reward_power=1\n",
    "                                                                      ),\n",
    "            \n",
    "            'reference_generator' : rg,\n",
    "            # define a numerical solver of adequate accuracy\n",
    "            'ode_solver' : 'euler' #'scipy.solve_ivp'\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "gym_env_kwargs2 = {'visualization': MotorDashboard(plots = ['i_sq', 'i_sd', 'reward']),\n",
    "            # parameterize the PMSM\n",
    "               'motor_parameter' : motor_parameter,\n",
    "               'limit_values' : limit_values,\n",
    "               'nominal_values' : nominal_values,\n",
    "               'u_sup' : u_sup,\n",
    "               'load' : ConstantSpeedLoad(omega_fixed=1000 * np.pi / 30),\n",
    "               #'state_filter' :['i_sq', 'i_sd', 'epsilon'],  # todo\n",
    "               #'reward_weights' : {'i_sq': 1000, 'i_sd': 1000},\n",
    "               #'reward_power' : 0.5,\n",
    "               #'observed_states' : ['i_sq', 'i_sd'],\n",
    "           \n",
    "            'tau' : 1e-5 ,\n",
    "                  \n",
    "                  ## pass a reward function with a gamma!!  todo\n",
    "            # turn off terminations via limit violation and parameterize the reward function\n",
    "            'reward_function' : gem.reward_functions.WeightedSumOfErrors(observed_states=['i_sq', 'i_sd'], \n",
    "                                                                        reward_weights={'i_sq': 1, 'i_sd': 1},\n",
    "                                                                        #constraint_monitor = SqdCurrentMonitor(),\n",
    "                                                                        gamma = gamma,\n",
    "                                                                        reward_power=1\n",
    "                                                                      ),\n",
    "            \n",
    "            'reference_generator' : rg,\n",
    "            # define a numerical solver of adequate accuracy\n",
    "            'ode_solver' : 'euler' #'scipy.solve_ivp'\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternate way to create a tf compatible gym env\n",
    "\n",
    "#t_env = gem.make(\"emotor-pmsm-disc-v1\", **gym_env_kwargs1)   # define a PMSM with continuous action space\n",
    "                \n",
    "# t_env_f = FlattenObservation(t_env) \n",
    "\n",
    "# t_py_env = suite_gym.wrap_env(t_env_f, max_episode_steps=1000)\n",
    "# t_tf_env = tf_py_environment.TFPyEnvironment(t_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FlattenObservation<DiscPermanentMagnetSynchronousMotorEnvironment<emotor-pmsm-disc-v1>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env_name = \"emotor-pmsm-disc-v1\"\n",
    "env = suite_gym.load(env_name, max_episode_steps=10000, gym_env_wrappers=[FlattenObservation],\n",
    "                      gym_kwargs = gym_env_kwargs1 )  #\n",
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "# #env = FlattenObservation(env)\n",
    "eval_py_env =suite_gym.load(env_name, max_episode_steps=10000, gym_env_wrappers=[FlattenObservation],\n",
    "                      gym_kwargs = gym_env_kwargs2) #gem.make(\"emotor-pmsm-disc-v1\", **gym_env_kwargs2)\n",
    "\n",
    "\n",
    "\n",
    "eval_tf_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "\n",
    "print(env.gym)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_env.time_step_spec())\n",
    "print('\\n \\n')\n",
    "eval_py_env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a neural network:\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "num_iterations = 5000 \n",
    "\n",
    "initial_collect_steps = 2000  \n",
    "collect_steps_per_iteration = 1 \n",
    "replay_buffer_max_length = 100000 \n",
    "\n",
    "batch_size = 64  \n",
    "learning_rate = 1e-4  \n",
    "log_interval = 200 \n",
    "\n",
    "num_eval_episodes = 10 \n",
    "eval_interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate DQN agent\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a replay buffer\n",
    "\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "import logging\n",
    "log_metrics(train_metrics)\n",
    "#logging.get_logger().set_level(logging.INFO)  \n",
    "logging.getLogger().setLevel(logging.INFO)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver to take action steps in the environment\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "\n",
    "collect_driver = DynamicStepDriver(\n",
    "    train_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/drivers/dynamic_step_driver.py:201: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/drivers/dynamic_step_driver.py:201: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000"
     ]
    }
   ],
   "source": [
    "# fill the replay buffer initially with trajectories from a random policy\n",
    "\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "\n",
    "initial_collect_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                        train_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    train_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch, ShowProgress(initial_collect_steps)],\n",
    "    num_steps=initial_collect_steps) \n",
    "final_time_step, final_policy_state = init_driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf_agents.trajectories import trajectory\n",
    "\n",
    "# def collect_step(environment, policy, buffer):\n",
    "#   time_step = environment.current_time_step()\n",
    "#   action_step = policy.action(time_step)\n",
    "#   next_time_step = environment.step(action_step.action)\n",
    "#   traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "#   # Add trajectory to the replay buffer\n",
    "#   buffer.add_batch(traj)\n",
    "\n",
    "# def collect_data(env, policy, buffer, steps):\n",
    "#   for _ in range(steps):\n",
    "#     collect_step(env, policy, buffer)\n",
    "\n",
    "# random_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
    "#                                                 train_env.action_spec())\n",
    "# collect_data(train_env, random_policy, replay_buffer, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f4b681b4400>\n"
     ]
    }
   ],
   "source": [
    "# dataset is sampled from the replay buffer\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset\n",
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 1\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986 loss:595.287662"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 9\n",
      "\t\t EnvironmentSteps = 1001\n",
      "\t\t AverageReturn = -312.27716064453125\n",
      "\t\t AverageEpisodeLength = 105.44444274902344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970 loss:530.899668"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 16\n",
      "\t\t EnvironmentSteps = 2001\n",
      "\t\t AverageReturn = -318.8725280761719\n",
      "\t\t AverageEpisodeLength = 123.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2957 loss:0.94341878"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 20\n",
      "\t\t EnvironmentSteps = 3001\n",
      "\t\t AverageReturn = -315.9631042480469\n",
      "\t\t AverageEpisodeLength = 175.39999389648438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3949 loss:0.37846476"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 24\n",
      "\t\t EnvironmentSteps = 4001\n",
      "\t\t AverageReturn = -323.8604431152344\n",
      "\t\t AverageEpisodeLength = 236.8000030517578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 loss:0.66962405"
     ]
    }
   ],
   "source": [
    "from tf_agents.utils.common import function\n",
    "\n",
    "agent.train = common.function(agent.train)\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "\n",
    "\n",
    "time_step = None\n",
    "policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
    "#iterator = iter(dataset)\n",
    "loss = []\n",
    "for iteration in range(num_iterations):\n",
    "    time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "    trajectories, buffer_info = next(iterator)\n",
    "    train_loss = agent.train(trajectories)\n",
    "    loss.append(train_loss.loss.numpy())\n",
    "    #t_env_f.render()\n",
    "    print(\"\\r{} loss:{:.5f}\".format(\n",
    "        iteration, train_loss.loss.numpy()), end=\"\")\n",
    "    if iteration % 1000 == 0:\n",
    "        log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4af859e4a8>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(0, num_iterations)\n",
    "plt.plot(x, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import base64\n",
    "import IPython\n",
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n",
      "array([[ 0.08333334,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        , -0.8888889 , -0.8888889 , -0.8888889 ,\n",
      "         0.        ,  0.        ,  0.        ,  1.        ,  0.7       ,\n",
      "         0.7       ]], dtype=float32)>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "append_data requires ndarray as first arg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-92c5dcda8ff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcreate_policy_eval_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trained-agent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-92c5dcda8ff5>\u001b[0m in \u001b[0;36mcreate_policy_eval_video\u001b[0;34m(policy, filename, num_episodes, fps)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0meval_py_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_py_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/imageio/core/format.py\u001b[0m in \u001b[0;36mappend_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;31m# Check image data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append_data requires ndarray as first arg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;31m# Get total meta dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mtotal_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: append_data requires ndarray as first arg"
     ]
    }
   ],
   "source": [
    "def create_policy_eval_video(policy, filename, num_episodes=1, fps=30):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = eval_tf_env.reset()\n",
    "        eval_py_env.render()\n",
    "        print(time_step)\n",
    "        video.append_data(eval_py_env.render())\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = eval_tf_env.step(action_step.action)\n",
    "            video.append_data(eval_py_env.render())\n",
    "            \n",
    "  return embed_mp4(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n",
      "array([[ 0.08333334,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        , -0.8888889 , -0.8888889 , -0.8888889 ,\n",
      "         0.        ,  0.        ,  0.        ,  1.        ,  0.7       ,\n",
      "         0.7       ]], dtype=float32)>) \n",
      " \n",
      "\n",
      "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(16,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "policy = agent.policy\n",
    "for _ in range(1):\n",
    "    time_step = eval_tf_env.reset()\n",
    "    print(time_step,'\\n \\n')\n",
    "    print(train_env.time_step_spec())\n",
    "    eval_py_env.render()\n",
    "    \n",
    "#     while not time_step.is_last():\n",
    "#         action_step = policy.action(time_step)\n",
    "#         time_step = eval_tf_env.step(action_step.action)\n",
    "#         eval_py_env.render()\n",
    "\n",
    "    action_step = policy.action(time_step)\n",
    "    time_step = eval_tf_env.step(action_step.action)\n",
    "    eval_py_env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
