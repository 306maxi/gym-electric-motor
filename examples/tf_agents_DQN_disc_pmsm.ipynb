{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym_electric_motor as gem\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from gym.wrappers import FlattenObservation\n",
    "#from tf_agents.environments.wrappers import FlattenObservationsWrapper\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from gym_electric_motor.visualization import MotorDashboard, ConsolePrinter\n",
    "from gym_electric_motor.physical_systems import ConstantSpeedLoad\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_generator = gem.reference_generators.WienerProcessReferenceGenerator(reference_state='i_sq')\n",
    "d_generator = gem.reference_generators.WienerProcessReferenceGenerator(reference_state='i_sd')\n",
    "rg = gem.reference_generators.MultipleReferenceGenerator([q_generator, d_generator])\n",
    "\n",
    "# Change of the default motor parameters.\n",
    "motor_parameter = dict(\n",
    "    r_s=15e-3, l_d=0.37e-3, l_q=1.2e-3, psi_p=65.6e-3, p=3, j_rotor=0.06\n",
    ")\n",
    "limit_values = dict(\n",
    "    i=160*1.41,\n",
    "    omega=12000 * np.pi / 30,\n",
    "    u=450\n",
    ")\n",
    "nominal_values = {key: 0.7 * limit for key, limit in limit_values.items()}\n",
    "u_sup = 400\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "\n",
    "gym_env_kwargs1 = {'visualization': MotorDashboard(plots = ['i_sq', 'i_sd', 'reward']),\n",
    "            # parameterize the PMSM\n",
    "               'motor_parameter' : motor_parameter,\n",
    "               'limit_values' : limit_values,\n",
    "               'nominal_values' : nominal_values,\n",
    "               'u_sup' : u_sup,\n",
    "               'load' : ConstantSpeedLoad(omega_fixed=1000 * np.pi / 30),\n",
    "               #'state_filter' :['i_sq', 'i_sd', 'epsilon'],  # todo\n",
    "               #'reward_weights' : {'i_sq': 1000, 'i_sd': 1000},\n",
    "               #'reward_power' : 0.5,\n",
    "               #'observed_states' : ['i_sq', 'i_sd'],\n",
    "           \n",
    "            'tau' : 1e-5 ,\n",
    "                  \n",
    "                  ## pass a reward function with a gamma!!  todo\n",
    "            # turn off terminations via limit violation and parameterize the reward function\n",
    "            'reward_function' : gem.reward_functions.WeightedSumOfErrors(observed_states=['i_sq', 'i_sd'], \n",
    "                                                                        reward_weights={'i_sq': 1, 'i_sd': 1},\n",
    "                                                                        #constraint_monitor = SqdCurrentMonitor(),\n",
    "                                                                        gamma = gamma,\n",
    "                                                                        reward_power=1\n",
    "                                                                      ),\n",
    "            \n",
    "            'reference_generator' : rg,\n",
    "            # define a numerical solver of adequate accuracy\n",
    "            'ode_solver' : 'euler' #'scipy.solve_ivp'\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "gym_env_kwargs2 = {'visualization': MotorDashboard(plots = ['i_sq', 'i_sd', 'reward']),\n",
    "            # parameterize the PMSM\n",
    "               'motor_parameter' : motor_parameter,\n",
    "               'limit_values' : limit_values,\n",
    "               'nominal_values' : nominal_values,\n",
    "               'u_sup' : u_sup,\n",
    "               'load' : ConstantSpeedLoad(omega_fixed=1000 * np.pi / 30),\n",
    "               #'state_filter' :['i_sq', 'i_sd', 'epsilon'],  # todo\n",
    "               #'reward_weights' : {'i_sq': 1000, 'i_sd': 1000},\n",
    "               #'reward_power' : 0.5,\n",
    "               #'observed_states' : ['i_sq', 'i_sd'],\n",
    "           \n",
    "            'tau' : 1e-5 ,\n",
    "                  \n",
    "                  ## pass a reward function with a gamma!!  todo\n",
    "            # turn off terminations via limit violation and parameterize the reward function\n",
    "            'reward_function' : gem.reward_functions.WeightedSumOfErrors(observed_states=['i_sq', 'i_sd'], \n",
    "                                                                        reward_weights={'i_sq': 1, 'i_sd': 1},\n",
    "                                                                        #constraint_monitor = SqdCurrentMonitor(),\n",
    "                                                                        gamma = gamma,\n",
    "                                                                        reward_power=1\n",
    "                                                                      ),\n",
    "            \n",
    "            'reference_generator' : rg,\n",
    "            # define a numerical solver of adequate accuracy\n",
    "            'ode_solver' : 'euler' #'scipy.solve_ivp'\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternate way to create a tf compatible gym env\n",
    "\n",
    "#t_env = gem.make(\"emotor-pmsm-disc-v1\", **gym_env_kwargs1)   # define a PMSM with continuous action space\n",
    "                \n",
    "# t_env_f = FlattenObservation(t_env) \n",
    "\n",
    "# t_py_env = suite_gym.wrap_env(t_env_f, max_episode_steps=1000)\n",
    "# t_tf_env = tf_py_environment.TFPyEnvironment(t_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FlattenObservation<DiscPermanentMagnetSynchronousMotorEnvironment<emotor-pmsm-disc-v1>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env_name = \"emotor-pmsm-disc-v1\"\n",
    "env = suite_gym.load(env_name, max_episode_steps=10000, gym_env_wrappers=[FlattenObservation],\n",
    "                      gym_kwargs = gym_env_kwargs1 )  #\n",
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "# #env = FlattenObservation(env)\n",
    "eval_py_env =suite_gym.load(env_name, max_episode_steps=10000, gym_env_wrappers=[FlattenObservation],\n",
    "                      gym_kwargs = gym_env_kwargs2) #gem.make(\"emotor-pmsm-disc-v1\", **gym_env_kwargs2)\n",
    "\n",
    "\n",
    "\n",
    "# eval_tf_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "\n",
    "print(env.gym)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a neural network:\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "num_iterations = 5000 \n",
    "\n",
    "initial_collect_steps = 2000  \n",
    "collect_steps_per_iteration = 1 \n",
    "replay_buffer_max_length = 100000 \n",
    "\n",
    "batch_size = 64  \n",
    "learning_rate = 1e-4  \n",
    "log_interval = 200 \n",
    "\n",
    "num_eval_episodes = 10 \n",
    "eval_interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate DQN agent\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a replay buffer\n",
    "\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "import logging\n",
    "log_metrics(train_metrics)\n",
    "#logging.get_logger().set_level(logging.INFO)  \n",
    "logging.getLogger().setLevel(logging.INFO)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver to take action steps in the environment\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "\n",
    "collect_driver = DynamicStepDriver(\n",
    "    train_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/drivers/dynamic_step_driver.py:201: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/praneeth/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/drivers/dynamic_step_driver.py:201: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000"
     ]
    }
   ],
   "source": [
    "# fill the replay buffer initially with trajectories from a random policy\n",
    "\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "\n",
    "initial_collect_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                        train_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    train_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch, ShowProgress(initial_collect_steps)],\n",
    "    num_steps=initial_collect_steps) \n",
    "final_time_step, final_policy_state = init_driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf_agents.trajectories import trajectory\n",
    "\n",
    "# def collect_step(environment, policy, buffer):\n",
    "#   time_step = environment.current_time_step()\n",
    "#   action_step = policy.action(time_step)\n",
    "#   next_time_step = environment.step(action_step.action)\n",
    "#   traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "#   # Add trajectory to the replay buffer\n",
    "#   buffer.add_batch(traj)\n",
    "\n",
    "# def collect_data(env, policy, buffer, steps):\n",
    "#   for _ in range(steps):\n",
    "#     collect_step(env, policy, buffer)\n",
    "\n",
    "# random_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
    "#                                                 train_env.action_spec())\n",
    "# collect_data(train_env, random_policy, replay_buffer, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f7a0c258940>\n"
     ]
    }
   ],
   "source": [
    "# dataset is sampled from the replay buffer\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset\n",
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 1\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982 loss:0.55778085"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 17\n",
      "\t\t EnvironmentSteps = 1001\n",
      "\t\t AverageReturn = -233.60791015625\n",
      "\t\t AverageEpisodeLength = 44.29999923706055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950 loss:0.35178799"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 26\n",
      "\t\t EnvironmentSteps = 2001\n",
      "\t\t AverageReturn = -296.56597900390625\n",
      "\t\t AverageEpisodeLength = 102.30000305175781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2962 loss:526.011849"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 32\n",
      "\t\t EnvironmentSteps = 3001\n",
      "\t\t AverageReturn = -333.84033203125\n",
      "\t\t AverageEpisodeLength = 142.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3986 loss:916.766850"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 36\n",
      "\t\t EnvironmentSteps = 4001\n",
      "\t\t AverageReturn = -378.08074951171875\n",
      "\t\t AverageEpisodeLength = 218.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 loss:1.17313657"
     ]
    }
   ],
   "source": [
    "from tf_agents.utils.common import function\n",
    "\n",
    "agent.train = common.function(agent.train)\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "\n",
    "\n",
    "time_step = None\n",
    "policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
    "#iterator = iter(dataset)\n",
    "loss = []\n",
    "for iteration in range(num_iterations):\n",
    "    time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "    trajectories, buffer_info = next(iterator)\n",
    "    train_loss = agent.train(trajectories)\n",
    "    loss.append(train_loss.loss.numpy())\n",
    "    #t_env_f.render()\n",
    "    print(\"\\r{} loss:{:.5f}\".format(\n",
    "        iteration, train_loss.loss.numpy()), end=\"\")\n",
    "    if iteration % 1000 == 0:\n",
    "        log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f79ec0b37b8>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(0, num_iterations)\n",
    "plt.plot(x, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.08333334,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  0.        , -0.8888889 , -0.8888889 , -0.8888889 ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.7       ,\n",
      "        0.7       ], dtype=float32)) \n",
      " \n",
      "\n",
      "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(16,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)))\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "In[0] is not a matrix. Instead it has shape [100] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-99bdb68ba706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#     time_step = eval_py_env.step(action_step.action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#     eval_py_env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \"\"\"\n\u001b[1;32m    478\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ppo_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/policies/greedy_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     distribution_step = self._wrapped_policy.distribution(\n\u001b[0;32m---> 77\u001b[0;31m         time_step, policy_state)\n\u001b[0m\u001b[1;32m     78\u001b[0m     return policy_step.PolicyStep(\n\u001b[1;32m     79\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0;31m# This here is set only for compatibility with info_spec in constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/policies/q_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     q_values, policy_state = self._q_network(\n\u001b[0;32m--> 123\u001b[0;31m         network_observation, time_step.step_type, policy_state)\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# TODO(b/122314058): Validate and enforce that sampling distributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnetwork_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tf_agents/networks/q_network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, observation, step_type, network_state, training)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         training=training)\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_q_value_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5575\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5576\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5577\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5578\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5579\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfagents/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix. Instead it has shape [100] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "\n",
    "policy = agent.policy\n",
    "for _ in range(5):\n",
    "    time_step = eval_py_env.reset()\n",
    "    print(time_step,'\\n \\n')\n",
    "    print(train_env.time_step_spec())\n",
    "    eval_py_env.render()\n",
    "    \n",
    "    while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "    #     time_step = eval_py_env.step(action_step.action)\n",
    "    #     eval_py_env.render()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
